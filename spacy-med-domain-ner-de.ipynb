{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "3.7.8\n2.3.2\n2.0.9\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "print(platform.python_version())\n",
    "print(spacy.__version__)\n",
    "print(json.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "def tsv_to_json(input_path, output_path, unknown_label):\n",
    "    try:\n",
    "        input_file = open(input_path, 'r', encoding='UTF-8')\n",
    "        output_file = open(output_path, 'w', encoding='UTF-8')\n",
    "        data_dict = {}\n",
    "        annotations = []\n",
    "        label_dict = {}\n",
    "        s = ''\n",
    "        start = 0\n",
    "        for line in input_file:\n",
    "            if line[0:len(line)-1] != '.\\tO':\n",
    "                word, entity = line.split('\\t')\n",
    "                s += word+\" \"\n",
    "                entity = entity[:len(entity)-1]\n",
    "                if entity != unknown_label and len(entity) != 1:\n",
    "                    d = {}\n",
    "                    d['text'] = word\n",
    "                    d['start'] = start\n",
    "                    d['end'] = start+len(word)-1\n",
    "                    try:\n",
    "                        label_dict[entity].append(d)\n",
    "                    except:\n",
    "                        label_dict[entity] = []\n",
    "                        label_dict[entity].append(d)\n",
    "                start += len(word)+1\n",
    "            else:\n",
    "                data_dict['content'] = s\n",
    "                s = ''\n",
    "                label_list = []\n",
    "                for ents in list(label_dict.keys()):\n",
    "                    for i in range(len(label_dict[ents])):\n",
    "                        if(label_dict[ents][i]['text'] != ''):\n",
    "                            l = [ents, label_dict[ents][i]]\n",
    "                            for j in range(i+1, len(label_dict[ents])):\n",
    "                                if(label_dict[ents][i]['text'] == label_dict[ents][j]['text']):\n",
    "                                    di = {}\n",
    "                                    di['start'] = label_dict[ents][j]['start']\n",
    "                                    di['end'] = label_dict[ents][j]['end']\n",
    "                                    di['text'] = label_dict[ents][i]['text']\n",
    "                                    l.append(di)\n",
    "                                    label_dict[ents][j]['text'] = ''\n",
    "                            label_list.append(l)\n",
    "\n",
    "                for entities in label_list:\n",
    "                    label = {}\n",
    "                    label['label'] = [entities[0]]\n",
    "                    label['points'] = entities[1:]\n",
    "                    annotations.append(label)\n",
    "                data_dict['annotation'] = annotations\n",
    "                annotations = []\n",
    "                json.dump(data_dict, output_file, ensure_ascii=False)\n",
    "                output_file.write('\\n')\n",
    "                data_dict = {}\n",
    "                label_dict = {}\n",
    "                start = 0                \n",
    "    except Exception as e:\n",
    "        logging.exception(\"Unable to process file\\nerror = \" + str(e))\n",
    "        raise e\n",
    "\n",
    "tsv_to_json(\"med-corpus.tsv\", 'med-corpus.json', 'abc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "import random\n",
    "# Read json file and transform it to array \n",
    "def transform_json(json_file_path):\n",
    "    corpus = open(json_file_path, 'r') \n",
    "    lines = corpus.readlines() \n",
    "  \n",
    "    training_data = [] #array of train data with marked up medical entities\n",
    "    count = 0\n",
    "    # read lines from file and parsing them into special array\n",
    "    for line in lines: \n",
    "        res = json.loads(line)   \n",
    "        text = res['content']\n",
    "        entities = []\n",
    "        for annotation in res['annotation']:\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                entities.append((point['start'], point['end'] + 1 ,label))\n",
    "        training_data.append((text, {\"entities\" : entities}))\n",
    "    return training_data\n",
    "training_data = transform_json('med-corpus.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Statring iteration 0\n",
      "{'ner': 967.3993598310248}\n",
      "Statring iteration 1\n",
      "{'ner': 464.7793052301019}\n",
      "Statring iteration 2\n",
      "{'ner': 323.11242348422314}\n",
      "Statring iteration 3\n",
      "{'ner': 224.23507634328385}\n",
      "Statring iteration 4\n",
      "{'ner': 181.11386628508345}\n",
      "Statring iteration 5\n",
      "{'ner': 151.26647821432104}\n",
      "Statring iteration 6\n",
      "{'ner': 131.36345293346355}\n",
      "Statring iteration 7\n",
      "{'ner': 70.64205344676871}\n",
      "Statring iteration 8\n",
      "{'ner': 62.13054990646648}\n",
      "Statring iteration 9\n",
      "{'ner': 73.58742452194633}\n",
      "Statring iteration 10\n",
      "{'ner': 79.40042562043193}\n",
      "Statring iteration 11\n",
      "{'ner': 122.54378101807364}\n",
      "Statring iteration 12\n",
      "{'ner': 60.725216368008624}\n",
      "Statring iteration 13\n",
      "{'ner': 60.85079054901087}\n",
      "Statring iteration 14\n",
      "{'ner': 59.25991630099659}\n",
      "Statring iteration 15\n",
      "{'ner': 54.13558897673037}\n",
      "Statring iteration 16\n",
      "{'ner': 43.39382800352808}\n",
      "Statring iteration 17\n",
      "{'ner': 45.852393317934144}\n",
      "Statring iteration 18\n",
      "{'ner': 57.0371787529803}\n",
      "Statring iteration 19\n",
      "{'ner': 37.60677288285902}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def train_spacy(data,iterations):    \n",
    "    nlp = spacy.blank('de')  # create blank Language class\n",
    "    # create the built-in pipeline components and add them to the pipeline\n",
    "    # nlp.create_pipe works for built-ins that are registered with spaCy\n",
    "    if 'ner' not in nlp.pipe_names:\n",
    "        ner = nlp.create_pipe('ner')\n",
    "        nlp.add_pipe(ner, last=True)\n",
    "       \n",
    "\n",
    "    # add labels\n",
    "    for _, annotations in data:\n",
    "         for ent in annotations.get('entities'):\n",
    "            ner.add_label(ent[2])\n",
    "\n",
    "    # get names of other pipes to disable them during training\n",
    "    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != 'ner']\n",
    "    with nlp.disable_pipes(*other_pipes):  # only train NER\n",
    "        optimizer = nlp.begin_training()\n",
    "        for itn in range(iterations):\n",
    "            print(\"Statring iteration \" + str(itn))\n",
    "            random.shuffle(data)\n",
    "            losses = {}\n",
    "            for text, annotations in data:\n",
    "                nlp.update(\n",
    "                    [text],  # batch of texts\n",
    "                    [annotations],  # batch of annotations\n",
    "                    drop=0.2,  # dropout - make it harder to memorise data\n",
    "                    sgd=optimizer,  # callable to update weights\n",
    "                    losses=losses)\n",
    "            print(losses)\n",
    "    return nlp\n",
    "\n",
    "\n",
    "prdnlp = train_spacy(training_data, 20)\n",
    "\n",
    "# Save our trained Model\n",
    "modelfile = 'models/' + input(\"Enter your Model Name: \")\n",
    "prdnlp.to_disk(modelfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Medikament 54 64 b-med\nPlacebos 116 124 b-med\nMedikamente 155 166 b-med\nWirkstoff 213 222 b-med\nWirkstoff 403 412 b-med\nKÃ¶rper 416 422 b-med\nWirkstoff 493 502 b-med\nZopiclon 779 787 b-med\n"
     ]
    }
   ],
   "source": [
    "#Test your text\n",
    "test_text = input(\"Enter your testing text: \")\n",
    "doc = prdnlp(test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "Python 3.7.8 64-bit",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "e534e48711db4d1e1c48977d0d14ff85b1f16d41bcc4fdfd88268a329b3c9d66"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}